{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#孤立森林法\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "# Generate train data\n",
    "X = 0.3 * rng.randn(100, 2)\n",
    "X_train = np.r_[X + 1, X - 3, X - 5, X + 6]\n",
    "# Generate some regular novel observations\n",
    "X = 0.3 * rng.randn(20, 2)\n",
    "X_test = np.r_[X + 1, X - 3, X - 5, X + 6]\n",
    "# Generate some abnormal novel observations\n",
    "X_outliers = rng.uniform(low=-8, high=8, size=(20, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class IsolationForest in module sklearn.ensemble.iforest:\n",
      "\n",
      "class IsolationForest(sklearn.ensemble.bagging.BaseBagging)\n",
      " |  Isolation Forest Algorithm\n",
      " |  \n",
      " |  Return the anomaly score of each sample using the IsolationForest algorithm\n",
      " |  \n",
      " |  The IsolationForest 'isolates' observations by randomly selecting a feature\n",
      " |  and then randomly selecting a split value between the maximum and minimum\n",
      " |  values of the selected feature.\n",
      " |  \n",
      " |  Since recursive partitioning can be represented by a tree structure, the\n",
      " |  number of splittings required to isolate a sample is equivalent to the path\n",
      " |  length from the root node to the terminating node.\n",
      " |  \n",
      " |  This path length, averaged over a forest of such random trees, is a\n",
      " |  measure of normality and our decision function.\n",
      " |  \n",
      " |  Random partitioning produces noticeably shorter paths for anomalies.\n",
      " |  Hence, when a forest of random trees collectively produce shorter path\n",
      " |  lengths for particular samples, they are highly likely to be anomalies.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <isolation_forest>`.\n",
      " |  \n",
      " |  .. versionadded:: 0.18\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, optional (default=100)\n",
      " |      The number of base estimators in the ensemble.\n",
      " |  \n",
      " |  max_samples : int or float, optional (default=\"auto\")\n",
      " |      The number of samples to draw from X to train each base estimator.\n",
      " |          - If int, then draw `max_samples` samples.\n",
      " |          - If float, then draw `max_samples * X.shape[0]` samples.\n",
      " |          - If \"auto\", then `max_samples=min(256, n_samples)`.\n",
      " |  \n",
      " |      If max_samples is larger than the number of samples provided,\n",
      " |      all samples will be used for all trees (no sampling).\n",
      " |  \n",
      " |  contamination : float in (0., 0.5), optional (default=0.1)\n",
      " |      The amount of contamination of the data set, i.e. the proportion\n",
      " |      of outliers in the data set. Used when fitting to define the threshold\n",
      " |      on the decision function.\n",
      " |  \n",
      " |  max_features : int or float, optional (default=1.0)\n",
      " |      The number of features to draw from X to train each base estimator.\n",
      " |  \n",
      " |          - If int, then draw `max_features` features.\n",
      " |          - If float, then draw `max_features * X.shape[1]` features.\n",
      " |  \n",
      " |  bootstrap : boolean, optional (default=False)\n",
      " |      If True, individual trees are fit on random subsets of the training\n",
      " |      data sampled with replacement. If False, sampling without replacement\n",
      " |      is performed.\n",
      " |  \n",
      " |  n_jobs : integer, optional (default=1)\n",
      " |      The number of jobs to run in parallel for both `fit` and `predict`.\n",
      " |      If -1, then the number of jobs is set to the number of cores.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  verbose : int, optional (default=0)\n",
      " |      Controls the verbosity of the tree building process.\n",
      " |  \n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  estimators_ : list of DecisionTreeClassifier\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  estimators_samples_ : list of arrays\n",
      " |      The subset of drawn samples (i.e., the in-bag samples) for each base\n",
      " |      estimator.\n",
      " |  \n",
      " |  max_samples_ : integer\n",
      " |      The actual number of samples\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\"\n",
      " |         Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on.\n",
      " |  .. [2] Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation-based\n",
      " |         anomaly detection.\" ACM Transactions on Knowledge Discovery from\n",
      " |         Data (TKDD) 6.1 (2012): 3.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      IsolationForest\n",
      " |      sklearn.ensemble.bagging.BaseBagging\n",
      " |      abc.NewBase\n",
      " |      sklearn.ensemble.base.BaseEnsemble\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, max_samples='auto', contamination=0.1, max_features=1.0, bootstrap=False, n_jobs=1, random_state=None, verbose=0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Average anomaly score of X of the base classifiers.\n",
      " |      \n",
      " |      The anomaly score of an input sample is computed as\n",
      " |      the mean anomaly score of the trees in the forest.\n",
      " |      \n",
      " |      The measure of normality of an observation given a tree is the depth\n",
      " |      of the leaf containing this observation, which is equivalent to\n",
      " |      the number of splittings required to isolate this point. In case of\n",
      " |      several observations n_left in the leaf, the average path length of\n",
      " |      a n_left samples isolation tree is added.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The training input samples. Sparse matrices are accepted only if\n",
      " |          they are supported by the base estimator.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      scores : array of shape (n_samples,)\n",
      " |          The anomaly score of the input samples.\n",
      " |          The lower, the more abnormal.\n",
      " |  \n",
      " |  fit(self, X, y=None, sample_weight=None)\n",
      " |      Fit estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          The input samples. Use ``dtype=np.float32`` for maximum\n",
      " |          efficiency. Sparse matrices are also supported, use sparse\n",
      " |          ``csc_matrix`` for maximum efficiency.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples] or None\n",
      " |          Sample weights. If None, then samples are equally weighted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Returns self.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict if a particular sample is an outlier or not.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      is_inlier : array, shape (n_samples,)\n",
      " |          For each observations, tells whether or not (+1 or -1) it should\n",
      " |          be considered as an inlier according to the fitted model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.ensemble.bagging.BaseBagging:\n",
      " |  \n",
      " |  estimators_samples_\n",
      " |      The subset of drawn samples for each base estimator.\n",
      " |      \n",
      " |      Returns a dynamically generated list of boolean masks identifying\n",
      " |      the samples used for fitting each member of the ensemble, i.e.,\n",
      " |      the in-bag samples.\n",
      " |      \n",
      " |      Note: the list is re-created at each call to the property in order\n",
      " |      to reduce the object memory footprint by not storing the sampling\n",
      " |      data. Thus fetching the property may be slower than expected.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble.base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Returns the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Returns iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Returns the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(IsolationForest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "clf = IsolationForest(max_samples=100*2, random_state=rng)\n",
    "clf.fit(X_train)\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "y_pred_outliers = clf.predict(X_outliers)\n",
    "\n",
    "# plot the line, the samples, and the nearest vectors to the plane\n",
    "xx, yy = np.meshgrid(np.linspace(-8, 8, 50), np.linspace(-8, 8, 50))\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"IsolationForest\")\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n",
    "\n",
    "b1 = plt.scatter(X_train[:, 0], X_train[:, 1], c='white')\n",
    "b2 = plt.scatter(X_test[:, 0], X_test[:, 1], c='green')\n",
    "c = plt.scatter(X_outliers[:, 0], X_outliers[:, 1], c='red')\n",
    "plt.axis('tight')\n",
    "plt.xlim((-8, 8))\n",
    "plt.ylim((-8, 8))\n",
    "plt.legend([b1, b2, c],\n",
    "           [\"training observations\",\n",
    "            \"new regular observations\", \"new abnormal observations\"],\n",
    "           loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1]\n",
      "[-1]\n"
     ]
    }
   ],
   "source": [
    "clf = IsolationForest()\n",
    "clf.fit(np.array([0, 1]).reshape(-1, 1))\n",
    "print(clf.predict([[0]]))\n",
    "print(clf.predict([[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict([[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Anoconda\\\\Anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\ensemble\\\\iforest.py'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import inspect\n",
    "# inspect.getsourcelines(IsolationForest)\n",
    "inspect.getsourcefile(IsolationForest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
